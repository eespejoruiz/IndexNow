{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bVw9uwUzDDoz",
        "hMTgSphZReYY",
        "GTAfk_LenE8x"
      ],
      "authorship_tag": "ABX9TyNIUwESS7jQCoLUheLLOwun",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eespejoruiz/IndexNow/blob/main/Index_Now_%7C_Amazon_Andes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) API Search Console Index Now\n"
      ],
      "metadata": {
        "id": "POMSn8A27gZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install git+https://github.com/antoineeripret/gsc_wrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpRBn1BDzkf0",
        "outputId": "0ee73433-3c15-45d7-ca23-5b9db7d6f727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/antoineeripret/gsc_wrapper\n",
            "  Cloning https://github.com/antoineeripret/gsc_wrapper to /tmp/pip-req-build-i1llh6la\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/antoineeripret/gsc_wrapper /tmp/pip-req-build-i1llh6la\n",
            "  Resolved https://github.com/antoineeripret/gsc_wrapper to commit 17186d74169c14f98e049df9e9746ca0b4d6d029\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from gscwrapper==0.0.7) (2.160.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from gscwrapper==0.0.7) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from gscwrapper==0.0.7) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from gscwrapper==0.0.7) (1.2.1)\n",
            "Collecting google.cloud==0.34.0 (from gscwrapper==0.0.7)\n",
            "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pandas==2.2.0 (from gscwrapper==0.0.7)\n",
            "  Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pandas_gbq==0.22.0 (from gscwrapper==0.0.7)\n",
            "  Downloading pandas_gbq-0.22.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting validators==0.23.2 (from gscwrapper==0.0.7)\n",
            "  Downloading validators-0.23.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting tqdm==4.66.1 (from gscwrapper==0.0.7)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prophet==1.1.5 (from gscwrapper==0.0.7)\n",
            "  Downloading prophet-1.1.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pycausalimpact==0.1.1 (from gscwrapper==0.0.7)\n",
            "  Downloading pycausalimpact-0.1.1-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting numpy==1.26.3 (from gscwrapper==0.0.7)\n",
            "  Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.31.0 (from gscwrapper==0.0.7)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.0->gscwrapper==0.0.7) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.0->gscwrapper==0.0.7) (2025.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (75.1.0)\n",
            "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.4.1)\n",
            "Requirement already satisfied: pyarrow>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (17.0.0)\n",
            "Requirement already satisfied: pydata-google-auth>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.9.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.10.2 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=3.3.5 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (3.25.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.11/dist-packages (from pandas_gbq==0.22.0->gscwrapper==0.0.7) (24.2)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5->gscwrapper==0.0.7) (1.2.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5->gscwrapper==0.0.7) (3.10.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5->gscwrapper==0.0.7) (0.66)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5->gscwrapper==0.0.7) (6.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pycausalimpact==0.1.1->gscwrapper==0.0.7) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pycausalimpact==0.1.1->gscwrapper==0.0.7) (0.14.4)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.11/dist-packages (from pycausalimpact==0.1.1->gscwrapper==0.0.7) (3.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->gscwrapper==0.0.7) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->gscwrapper==0.0.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->gscwrapper==0.0.7) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->gscwrapper==0.0.7) (2025.1.31)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.3->gscwrapper==0.0.7) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.3->gscwrapper==0.0.7) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.3->gscwrapper==0.0.7) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.13.0->gscwrapper==0.0.7) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.13.0->gscwrapper==0.0.7) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.13.0->gscwrapper==0.0.7) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.2.0->gscwrapper==0.0.7) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.3->gscwrapper==0.0.7) (1.17.0)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet==1.1.5->gscwrapper==0.0.7) (0.5.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.67.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq==0.22.0->gscwrapper==0.0.7) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.26.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery<4.0.0dev,>=3.3.5->pandas_gbq==0.22.0->gscwrapper==0.0.7) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery<4.0.0dev,>=3.3.5->pandas_gbq==0.22.0->gscwrapper==0.0.7) (2.7.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.7.3->gscwrapper==0.0.7) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10->pycausalimpact==0.1.1->gscwrapper==0.0.7) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5->gscwrapper==0.0.7) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5->gscwrapper==0.0.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5->gscwrapper==0.0.7) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5->gscwrapper==0.0.7) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5->gscwrapper==0.0.7) (11.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.13.0->gscwrapper==0.0.7) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.2.0->gscwrapper==0.0.7) (3.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11.0->pycausalimpact==0.1.1->gscwrapper==0.0.7) (1.0.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery<4.0.0dev,>=3.3.5->pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery<4.0.0dev,>=3.3.5->pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.62.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=3.3.5->pandas_gbq==0.22.0->gscwrapper==0.0.7) (1.6.0)\n",
            "Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
            "Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_gbq-0.22.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading prophet-1.1.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycausalimpact-0.1.1-py2.py3-none-any.whl (30 kB)\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.23.2-py3-none-any.whl (27 kB)\n",
            "Building wheels for collected packages: gscwrapper\n",
            "  Building wheel for gscwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gscwrapper: filename=gscwrapper-0.0.7-py3-none-any.whl size=99505 sha256=d92e1b5340b853437f1f02e75b2518fb071c1892fba69d886166f872a3cc7f46\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4et617h8/wheels/35/0e/53/3a99dc4f8d779253bc70f334f09be47a54e23e117f6ac993ff\n",
            "Successfully built gscwrapper\n",
            "Installing collected packages: google.cloud, validators, tqdm, requests, numpy, pandas, pycausalimpact, prophet, pandas_gbq, gscwrapper\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: prophet\n",
            "    Found existing installation: prophet 1.1.6\n",
            "    Uninstalling prophet-1.1.6:\n",
            "      Successfully uninstalled prophet-1.1.6\n",
            "  Attempting uninstall: pandas_gbq\n",
            "    Found existing installation: pandas-gbq 0.26.1\n",
            "    Uninstalling pandas-gbq-0.26.1:\n",
            "      Successfully uninstalled pandas-gbq-0.26.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "bigframes 1.36.0 requires pandas-gbq>=0.26.0, but you have pandas-gbq 0.22.0 which is incompatible.\n",
            "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.26.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google.cloud-0.34.0 gscwrapper-0.0.7 numpy-1.26.3 pandas-2.2.0 pandas_gbq-0.22.0 prophet-1.1.5 pycausalimpact-0.1.1 requests-2.31.0 tqdm-4.66.1 validators-0.23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Instalo Librer√≠as"
      ],
      "metadata": {
        "id": "csJ5UTUb7n8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgb54FDQymyb"
      },
      "outputs": [],
      "source": [
        "import gscwrapper\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gscwrapper.account import Account"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gscwrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-VaHdLf6nM0",
        "outputId": "c4d24fcc-d8b2-4ab1-fb88-5882fb35cb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gscwrapper\n",
            "Version: 0.0.7\n",
            "Summary: A simple wrapper for Google Search Console API for SEO data analysis.\n",
            "Home-page: https://github.com/antoineeripret/gsc_wrapper\n",
            "Author: Antoine Eripret\n",
            "Author-email: antoine.eripret.dev@gmail.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: google-api-python-client, google-auth, google-auth-oauthlib, google.cloud, numpy, pandas, pandas-gbq, prophet, pycausalimpact, python-dateutil, requests, tqdm, validators\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Me conecto a google drive\n"
      ],
      "metadata": {
        "id": "zauqaIT52260"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp6UXgcv28Ho",
        "outputId": "74b316b1-2e6f-474c-c51d-35a8d5aeae08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Me Conecto al API y Selecciono de Propiedad en Google Search Console\n"
      ],
      "metadata": {
        "id": "GPjsPsaf8Wa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account_service = (\n",
        "    gscwrapper\n",
        "    .generate_auth(\n",
        "        client_config=\"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\",\n",
        "        service_account_auth=True\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "vdtz5rZTUCxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "\n",
        "# üìå Definir el `scope` correcto para la API de Indexing\n",
        "SCOPES = [\"https://www.googleapis.com/auth/indexing\"]\n",
        "\n",
        "def autenticar_con_cuenta_de_servicio():\n",
        "    \"\"\"Autentica con una cuenta de servicio sin intervenci√≥n manual.\"\"\"\n",
        "\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        credenciales_json,\n",
        "        scopes=SCOPES\n",
        "    )\n",
        "\n",
        "    return creds\n",
        "\n",
        "# üîπ Ejecutar la autenticaci√≥n con la cuenta de servicio\n",
        "credentials = autenticar_con_cuenta_de_servicio()\n",
        "\n",
        "# üîπ Crear servicio para la API de Indexing con los permisos correctos\n",
        "service = build(\"indexing\", \"v3\", credentials=credentials)\n",
        "\n",
        "print(\"‚úÖ Autenticaci√≥n exitosa con cuenta de servicio. 'service' est√° listo para Indexing API.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPWND5KCV8P7",
        "outputId": "57294303-dc10-46eb-e87b-41ce1bd134f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Autenticaci√≥n exitosa con cuenta de servicio. 'service' est√° listo para Indexing API.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "account_service"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Ns75RUUc77",
        "outputId": "0f645ecd-c0c2-4346-ce92-5d67e0aa70a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<searchconsole.account.Account>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Extraer URLs del Sitemap"
      ],
      "metadata": {
        "id": "-zJYI5nZ8Lrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import gzip\n",
        "import io\n",
        "import time\n",
        "\n",
        "# üìå URLs base\n",
        "sitemap_index_url = \"https://amazon-andes.com/sitemap_index.xml\"\n",
        "ruta_indexadas = \"/content/drive/MyDrive/000 | Amazon Andes/urls_indexadas.txt\"\n",
        "ruta_por_indexar = \"/content/drive/MyDrive/000 | Amazon Andes/urls_por_indexar.txt\"\n",
        "\n",
        "def obtener_sitemaps(url):\n",
        "    \"\"\"Obtiene la lista de archivos sitemap desde el sitemap index con reintentos en caso de fallo.\"\"\"\n",
        "    for intento in range(3):  # Reintentar hasta 3 veces\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Verificar si el contenido es XML v√°lido\n",
        "            if \"xml\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "                root = ET.fromstring(response.content)\n",
        "                sitemaps = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                return sitemaps\n",
        "            else:\n",
        "                print(\"‚ùå El contenido recibido no es XML v√°lido.\")\n",
        "                return []\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ö†Ô∏è Error al obtener el sitemap index. Intento {intento + 1}/3: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(\"‚ùå No se pudo obtener el sitemap index despu√©s de 3 intentos.\")\n",
        "    return []\n",
        "\n",
        "def obtener_urls_de_sitemaps(lista_sitemaps):\n",
        "    \"\"\"Escanea todos los sitemaps y extrae las URLs, descomprimiendo archivos .gz si es necesario.\"\"\"\n",
        "    urls_totales = set()\n",
        "\n",
        "    for sitemap in lista_sitemaps:\n",
        "        for intento in range(3):  # Reintentar hasta 3 veces en caso de error\n",
        "            try:\n",
        "                response = requests.get(sitemap, timeout=10)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Si el archivo es .xml.gz, descomprimirlo antes de procesarlo\n",
        "                if sitemap.endswith(\".gz\"):\n",
        "                    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
        "                        xml_content = f.read().decode(\"utf-8\")\n",
        "                else:\n",
        "                    xml_content = response.content.decode(\"utf-8\")\n",
        "\n",
        "                # Validar si el XML es v√°lido\n",
        "                if not xml_content.strip().startswith(\"<\"):\n",
        "                    print(f\"‚ö†Ô∏è {sitemap} no contiene XML v√°lido.\")\n",
        "                    continue\n",
        "\n",
        "                # Parsear el XML\n",
        "                root = ET.fromstring(xml_content)\n",
        "                urls = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                urls_totales.update(urls)\n",
        "                break  # Salir del bucle si el intento fue exitoso\n",
        "\n",
        "            except (requests.exceptions.RequestException, ET.ParseError) as e:\n",
        "                print(f\"‚ö†Ô∏è Error en intento {intento + 1}/3 con {sitemap}: {e}\")\n",
        "                time.sleep(2)  # Esperar antes de reintentar\n",
        "\n",
        "    return urls_totales\n",
        "\n",
        "def leer_urls_indexadas(ruta):\n",
        "    \"\"\"Lee las URLs ya indexadas desde el archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return set(f.read().splitlines())\n",
        "    return set()\n",
        "\n",
        "def guardar_urls_nuevas(ruta, urls_nuevas):\n",
        "    \"\"\"Guarda solo las nuevas URLs en el archivo (sobrescribe el archivo anterior).\"\"\"\n",
        "    if urls_nuevas:\n",
        "        with open(ruta, \"w\") as f:  # ‚ö†Ô∏è 'w' para sobrescribir el archivo anterior\n",
        "            for url in urls_nuevas:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚úÖ {len(urls_nuevas)} nuevas URLs guardadas en '{ruta}'.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No hay nuevas URLs para agregar.\")\n",
        "\n",
        "def actualizar_urls_indexadas(ruta, urls_nuevas):\n",
        "    \"\"\"Agrega las nuevas URLs indexadas al archivo `urls_indexadas.txt`.\"\"\"\n",
        "    if urls_nuevas:\n",
        "        with open(ruta, \"a\") as f:  # ‚ö†Ô∏è 'a' para agregar nuevas URLs sin borrar las anteriores\n",
        "            for url in urls_nuevas:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚úÖ {len(urls_nuevas)} URLs a√±adidas a '{ruta}' para futuras referencias.\")\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Obtener todos los archivos de sitemap desde el sitemap index\n",
        "sitemaps_encontrados = obtener_sitemaps(sitemap_index_url)\n",
        "print(f\"üîç Se encontraron {len(sitemaps_encontrados)} sitemaps.\")\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Extraer todas las URLs de los sitemaps\n",
        "urls_extraidas = obtener_urls_de_sitemaps(sitemaps_encontrados)\n",
        "print(f\"‚úÖ Total de URLs extra√≠das: {len(urls_extraidas)}\")\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Leer el archivo de URLs indexadas y filtrar las nuevas\n",
        "urls_indexadas = leer_urls_indexadas(ruta_indexadas)\n",
        "urls_nuevas = urls_extraidas - urls_indexadas\n",
        "\n",
        "# üîπ 4Ô∏è‚É£ Guardar solo las nuevas URLs en `urls_por_indexar.txt` para ser enviadas\n",
        "guardar_urls_nuevas(ruta_por_indexar, urls_nuevas)\n",
        "\n",
        "# üîπ 5Ô∏è‚É£ Actualizar `urls_indexadas.txt` con las nuevas URLs enviadas\n",
        "actualizar_urls_indexadas(ruta_indexadas, urls_nuevas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZXAFt_D3XhE",
        "outputId": "df33d1bb-5b9a-4807-d9c5-09a4da511c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Se encontraron 12 sitemaps.\n",
            "‚úÖ Total de URLs extra√≠das: 415\n",
            "‚úÖ 1 nuevas URLs guardadas en '/content/drive/MyDrive/000 | Amazon Andes/urls_por_indexar.txt'.\n",
            "‚úÖ 1 URLs a√±adidas a '/content/drive/MyDrive/000 | Amazon Andes/urls_indexadas.txt' para futuras referencias.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Enviar las URLs Nuevas a Google Search Console"
      ],
      "metadata": {
        "id": "LDT7hQqiSbv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# üìå Ruta del archivo donde est√°n las URLs a indexar\n",
        "ruta_urls = \"/content/drive/MyDrive/000 | Amazon Andes/urls_por_indexar.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 | Amazon Andes/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo y las devuelve como una lista.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google y muestra el estado del env√≠o.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "\n",
        "        # üìå Verificar si la API respondi√≥ correctamente\n",
        "        if \"urlNotificationMetadata\" in response:\n",
        "            print(f\"‚úÖ URL enviada con √©xito: {url}\")\n",
        "            print(f\"üîπ √öltima actualizaci√≥n: {response['urlNotificationMetadata'].get('latestUpdate', 'No disponible')}\\n\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è URL enviada pero sin confirmaci√≥n clara: {url}\")\n",
        "            print(f\"üîπ Respuesta: {response}\\n\")\n",
        "\n",
        "        return True  # Indica que la URL se envi√≥ correctamente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\\n\")\n",
        "        return False  # Indica que hubo un error\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que no pudieron enviarse para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"w\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "# üîπ Leer las URLs desde el archivo\n",
        "urls_a_indexar = leer_urls(ruta_urls)\n",
        "\n",
        "if not urls_a_indexar:\n",
        "    print(\"‚ö†Ô∏è No hay URLs en el archivo para indexar.\")\n",
        "else:\n",
        "    print(f\"üöÄ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "\n",
        "    # üîπ Lista de URLs que fallaron al enviarse\n",
        "    urls_fallidas = []\n",
        "\n",
        "    # üîπ Enviar todas las URLs a Google Indexing API\n",
        "    for url in urls_a_indexar:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_fallidas.append(url)\n",
        "\n",
        "        time.sleep(2)  # üîπ Pausa para evitar bloqueos\n",
        "\n",
        "    # üîπ Guardar las URLs con errores para reintento\n",
        "    guardar_urls_fallidas(urls_fallidas)\n",
        "\n",
        "    print(\"‚úÖ Proceso de env√≠o completado.\")"
      ],
      "metadata": {
        "id": "mZxOBAUbSfhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9203dc1-e9ce-4995-c54a-64200300be0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Enviando 1 URLs a Google Indexing API...\n",
            "‚úÖ URL enviada con √©xito: https://amazon-andes.com/product/muna-essential-oil-peru/\n",
            "üîπ √öltima actualizaci√≥n: No disponible\n",
            "\n",
            "‚úÖ Proceso de env√≠o completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî∫ Enviar las TODAS las URLs a Google Search Console | Usar solo en casos de cambios generales en la web."
      ],
      "metadata": {
        "id": "bVw9uwUzDDoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Archivos de URLs\n",
        "ruta_urls = \"/content/drive/MyDrive/000 | Amazon Andes/urls_indexadas.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 | Amazon Andes/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que fallaron en un archivo para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"a\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada: {url} - Respuesta: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Leer todas las URLs desde el archivo\n",
        "urls_a_indexar = leer_urls(ruta_urls)\n",
        "\n",
        "# Filtrar para enviar solo 2000 por d√≠a\n",
        "urls_a_indexar = urls_a_indexar[:2000]\n",
        "\n",
        "# Lista de URLs que no pudieron enviarse\n",
        "urls_fallidas = []\n",
        "\n",
        "if urls_a_indexar:\n",
        "    print(f\"üîπ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "    for url in urls_a_indexar:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_fallidas.append(url)\n",
        "        time.sleep(2)  # Pausa para evitar bloqueos\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay URLs en el archivo para indexar.\")\n",
        "\n",
        "# Guardar las URLs con errores\n",
        "guardar_urls_fallidas(urls_fallidas)"
      ],
      "metadata": {
        "id": "N_loQkaiCj5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå C√≥digo: Reintentar el env√≠o de URLs fallidas"
      ],
      "metadata": {
        "id": "hMTgSphZReYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Archivo con las URLs que fallaron en intentos anteriores\n",
        "ruta_errores = \"/content/drive/MyDrive/000 | Expreso/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Sobrescribe el archivo con las URLs que a√∫n fallan.\"\"\"\n",
        "    with open(ruta_errores, \"w\") as f:\n",
        "        for url in urls:\n",
        "            f.write(url + \"\\n\")\n",
        "    print(f\"‚ö†Ô∏è {len(urls)} URLs siguen fallando y se guardaron en '{ruta_errores}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada: {url} - Respuesta: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Leer las URLs fallidas\n",
        "urls_fallidas = leer_urls(ruta_errores)\n",
        "\n",
        "# Lista de URLs que sigan fallando despu√©s del reintento\n",
        "urls_errores_nuevos = []\n",
        "\n",
        "if urls_fallidas:\n",
        "    print(f\"üîπ Reintentando enviar {len(urls_fallidas)} URLs a Google Indexing API...\")\n",
        "    for url in urls_fallidas:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_errores_nuevos.append(url)  # Guardar solo las que siguen fallando\n",
        "        time.sleep(2)  # Pausa para evitar bloqueos\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay URLs fallidas para reintentar.\")\n",
        "\n",
        "# Guardar las URLs que a√∫n fallan\n",
        "guardar_urls_fallidas(urls_errores_nuevos)"
      ],
      "metadata": {
        "id": "hSh5mEXBRg1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ Verificar y Enviar a Indexar las URLs No Indexadas\n",
        "\n"
      ],
      "metadata": {
        "id": "GTAfk_LenE8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IxMDbD83LqP",
        "outputId": "184c2fe0-113f-4612-f78f-baa6c99222ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "\n",
        "# üìå Definir los permisos (scopes) correctos\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters\", \"https://www.googleapis.com/auth/indexing\"]\n",
        "\n",
        "# üîπ Autenticar con Google Search Console API\n",
        "credentials = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "service_indexing = build(\"indexing\", \"v3\", credentials=credentials)\n",
        "\n",
        "# üîπ Verificar autenticaci√≥n exitosa\n",
        "print(\"‚úÖ Autenticaci√≥n exitosa con Google Search Console API y Google Indexing API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNBu9w7y3h1M",
        "outputId": "7f12e21f-87a6-4d69-dd53-1d22bf07a34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Autenticaci√≥n exitosa con Google Search Console API y Google Indexing API\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "\n",
        "# üìå Definir los permisos (scopes) correctos\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "\n",
        "# üîπ Autenticar con Google Search Console API\n",
        "credentials = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "\n",
        "# üîπ Obtener la lista de sitios que la cuenta de servicio puede acceder\n",
        "sites_list = service_gsc.sites().list().execute()\n",
        "\n",
        "# üîπ Mostrar los sitios a los que tiene acceso\n",
        "for site in sites_list.get(\"siteEntry\", []):\n",
        "    print(f\"üîπ Acceso a: {site['siteUrl']} - Rol: {site['permissionLevel']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUI0nMst5z8A",
        "outputId": "83831f9e-d6f2-4457-f662-e3bc330755fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Acceso a: sc-domain:palosantopremium.com - Rol: siteOwner\n",
            "üîπ Acceso a: sc-domain:amazon-andes.com - Rol: siteOwner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# üìå URL del sitemap indexado\n",
        "sitemap_index_url = \"https://amazon-andes.com/sitemap_index.xml\"\n",
        "\n",
        "# üìå Archivos de salida\n",
        "output_csv = \"/content/drive/MyDrive/000 | Amazon Andes/resultados_indexacion.csv\"\n",
        "urls_no_indexadas_txt = \"/content/drive/MyDrive/000 | Amazon Andes/urls_no_indexadas.txt\"\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Obtener todos los sitemaps desde `sitemap-index.xml`\n",
        "def obtener_sitemaps(url_sitemap_index):\n",
        "    response = requests.get(url_sitemap_index)\n",
        "    sitemaps = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        root = ET.fromstring(response.content)\n",
        "        for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
        "            sitemaps.append(elem.text)\n",
        "    else:\n",
        "        print(\"‚ùå Error al obtener el sitemap indexado\")\n",
        "\n",
        "    return sitemaps\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Obtener todas las URLs de cada sitemap\n",
        "def obtener_urls_de_sitemaps(lista_sitemaps):\n",
        "    urls_totales = []\n",
        "\n",
        "    for sitemap in lista_sitemaps:\n",
        "        response = requests.get(sitemap)\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
        "                urls_totales.append(elem.text)\n",
        "        else:\n",
        "            print(f\"‚ùå Error al obtener {sitemap}\")\n",
        "\n",
        "    return urls_totales\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Consultar si una URL est√° indexada en Google Search Console\n",
        "def verificar_indexacion(service_gsc, url, site_url):\n",
        "    try:\n",
        "        request = {\n",
        "            \"startDate\": \"2024-01-01\",\n",
        "            \"endDate\": \"2025-02-16\",\n",
        "            \"dimensions\": [\"page\"],\n",
        "            \"dimensionFilterGroups\": [{\n",
        "                \"filters\": [{\n",
        "                    \"dimension\": \"page\",\n",
        "                    \"operator\": \"equals\",\n",
        "                    \"expression\": url\n",
        "                }]\n",
        "            }],\n",
        "            \"rowLimit\": 1\n",
        "        }\n",
        "\n",
        "        # üîπ Consultar la API de GSC\n",
        "        response = service_gsc.searchanalytics().query(siteUrl=site_url, body=request).execute()\n",
        "\n",
        "        if \"rows\" in response and len(response[\"rows\"]) > 0:\n",
        "            return \"‚úÖ Indexada\"\n",
        "        else:\n",
        "            return \"‚ùå No indexada\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error con {url}: {e}\")\n",
        "        return \"‚ö†Ô∏è Error\"\n",
        "\n",
        "# üîπ 4Ô∏è‚É£ Ejecutar la verificaci√≥n de indexaci√≥n\n",
        "def analizar_indexacion():\n",
        "    site_url = \"sc-domain:amazon-andes.com\"  # ‚ö†Ô∏è Usa la versi√≥n exacta registrada en GSC\n",
        "    print(f\"üîπ Obteniendo lista de sitemaps desde: {sitemap_index_url}\")\n",
        "    lista_sitemaps = obtener_sitemaps(sitemap_index_url)\n",
        "\n",
        "    urls_sitemap = obtener_urls_de_sitemaps(lista_sitemaps)\n",
        "    resultados = []\n",
        "    urls_no_indexadas = []\n",
        "\n",
        "    print(f\"üîπ Verificando indexaci√≥n de {len(urls_sitemap)} URLs en Google Search Console...\")\n",
        "\n",
        "    for url in urls_sitemap:\n",
        "        estado = verificar_indexacion(service_gsc, url, site_url)\n",
        "        resultados.append([url, estado])\n",
        "\n",
        "        if estado == \"‚ùå No indexada\":\n",
        "            urls_no_indexadas.append(url)\n",
        "\n",
        "        time.sleep(2)  # üîπ Pausa para evitar l√≠mites de la API\n",
        "\n",
        "    # Guardar los resultados\n",
        "    with open(output_csv, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"URL\", \"Estado de Indexaci√≥n\"])\n",
        "        writer.writerows(resultados)\n",
        "\n",
        "    with open(urls_no_indexadas_txt, \"w\") as f:\n",
        "        for url in urls_no_indexadas:\n",
        "            f.write(url + \"\\n\")\n",
        "\n",
        "    print(\"‚úÖ An√°lisis de indexaci√≥n completado.\")\n",
        "\n",
        "# üîπ Ejecutar la verificaci√≥n de indexaci√≥n\n",
        "analizar_indexacion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxDynEDD3szS",
        "outputId": "1421d594-7a7c-44bc-cdf5-8dc694939bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Obteniendo lista de sitemaps desde: https://amazon-andes.com/sitemap_index.xml\n",
            "üîπ Verificando indexaci√≥n de 415 URLs en Google Search Console...\n",
            "‚úÖ An√°lisis de indexaci√≥n completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "\n",
        "# üìå Definir el permiso (scope) correcto\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "\n",
        "# üîπ Autenticar con Google Search Console API\n",
        "credentials = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "\n",
        "# üîπ Obtener la lista de sitios que la cuenta de servicio puede acceder\n",
        "sites_list = service_gsc.sites().list().execute()\n",
        "\n",
        "# üîπ Mostrar los sitios a los que tiene acceso\n",
        "print(\"üîπ La cuenta de servicio tiene acceso a:\")\n",
        "for site in sites_list.get(\"siteEntry\", []):\n",
        "    print(f\" - {site['siteUrl']} | Permiso: {site['permissionLevel']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JxZQJLUN0Tm",
        "outputId": "1788a0b4-5b3e-41e0-f487-f39b4bc2505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ La cuenta de servicio tiene acceso a:\n",
            " - sc-domain:palosantopremium.com | Permiso: siteOwner\n",
            " - sc-domain:amazon-andes.com | Permiso: siteOwner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# üìå Archivos de entrada/salida\n",
        "ruta_urls_no_indexadas = \"/content/drive/MyDrive/000 | Amazon Andes/urls_no_indexadas.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 | Amazon Andes/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls_no_indexadas(ruta):\n",
        "    \"\"\"Lee las URLs no indexadas desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que fallaron en un archivo para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"w\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "def actualizar_archivo_urls(urls_exitosas):\n",
        "    \"\"\"Elimina del archivo las URLs que ya fueron indexadas correctamente.\"\"\"\n",
        "    if not os.path.exists(ruta_urls_no_indexadas):\n",
        "        return\n",
        "\n",
        "    urls_actuales = leer_urls_no_indexadas(ruta_urls_no_indexadas)\n",
        "    nuevas_urls = list(set(urls_actuales) - set(urls_exitosas))  # üîπ Filtrar URLs exitosas\n",
        "\n",
        "    with open(ruta_urls_no_indexadas, \"w\") as f:\n",
        "        for url in nuevas_urls:\n",
        "            f.write(url + \"\\n\")\n",
        "\n",
        "    print(f\"üìÑ Archivo actualizado: {len(nuevas_urls)} URLs restantes en '{ruta_urls_no_indexadas}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service_indexing.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada a indexar: {url}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# üîπ Leer URLs a indexar\n",
        "urls_a_indexar = leer_urls_no_indexadas(ruta_urls_no_indexadas)\n",
        "urls_fallidas = []\n",
        "urls_indexadas = []\n",
        "\n",
        "if urls_a_indexar:\n",
        "    print(f\"üîπ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "    for url in urls_a_indexar:\n",
        "        if enviar_url_para_indexacion(url):\n",
        "            urls_indexadas.append(url)  # Guardamos las exitosas\n",
        "        else:\n",
        "            urls_fallidas.append(url)  # Guardamos las fallidas\n",
        "        time.sleep(2)  # üîπ Evitar bloqueos con la API\n",
        "\n",
        "# üîπ Actualizar archivos\n",
        "guardar_urls_fallidas(urls_fallidas)\n",
        "actualizar_archivo_urls(urls_indexadas)"
      ],
      "metadata": {
        "id": "5YLlZ9etMDvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corregir errores de Indexaci√≥n"
      ],
      "metadata": {
        "id": "wXvwFwSyH9E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå PASO 1: Instalar Dependencias"
      ],
      "metadata": {
        "id": "1FvjjXFXIHgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt2UnvIUIDnV",
        "outputId": "99cbe605-160a-4caf-a074-3a09e06d25d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.11/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.67.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå PASO 2: Autenticaci√≥n con Google Search Console API"
      ],
      "metadata": {
        "id": "uluFG4YCIIgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "\n",
        "# üìå Definir el permiso para GSC\n",
        "SCOPES_GSC = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "credentials_gsc = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES_GSC)\n",
        "\n",
        "# üîπ Autenticaci√≥n con Google Search Console API\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials_gsc)\n",
        "\n",
        "print(\"‚úÖ Autenticaci√≥n exitosa con Google Search Console API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "x1G_xUsxIK17",
        "outputId": "cd80dc43-701f-4ba2-f188-e698aac375e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3dc1ffdd9be1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# üìå Definir el permiso para GSC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mSCOPES_GSC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"https://www.googleapis.com/auth/webmasters.readonly\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcredentials_gsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_service_account_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredenciales_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCOPES_GSC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# üîπ Autenticaci√≥n con Google Search Console API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/oauth2/service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[0;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m         info, signer = _service_account_info.from_filename(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"client_email\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_uri\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[0;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0msigner\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \"\"\"\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rsa_signer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_rsa_signer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå PASO 3: Obtener y Analizar Errores de Indexaci√≥n:\n",
        "Este script extrae los errores de indexaci√≥n de Google Search Console y los guarda en un CSV."
      ],
      "metadata": {
        "id": "ES32wpNjIR7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# üìå Definir la URL del Sitemap Index\n",
        "sitemap_index_url = \"https://amazon-andes.com/sitemap_index.xml\"\n",
        "\n",
        "# üìå Directorio donde se guardar√°n los archivos de errores\n",
        "output_dir = \"/content/drive/MyDrive/000 | Amazon Andes/errores_indexacion/\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Crear la carpeta si no existe\n",
        "\n",
        "def obtener_urls_sitemap(url_sitemap):\n",
        "    \"\"\"Obtiene todas las URLs de los sitemaps del sitio.\"\"\"\n",
        "    urls_totales = set()\n",
        "    response = requests.get(url_sitemap)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        root = ET.fromstring(response.content)\n",
        "\n",
        "        # üìå Buscar archivos sitemap dentro del √≠ndice\n",
        "        sitemaps = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "\n",
        "        for sitemap in sitemaps:\n",
        "            response_sitemap = requests.get(sitemap)\n",
        "            if response_sitemap.status_code == 200:\n",
        "                root_sitemap = ET.fromstring(response_sitemap.content)\n",
        "                urls = [elem.text for elem in root_sitemap.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                urls_totales.update(urls)\n",
        "    else:\n",
        "        print(f\"‚ùå Error al obtener el sitemap indexado: {response.status_code}\")\n",
        "\n",
        "    return list(urls_totales)\n",
        "\n",
        "def obtener_y_clasificar_errores():\n",
        "    \"\"\"Obtiene los errores de indexaci√≥n usando Site Inspection API y los clasifica din√°micamente.\"\"\"\n",
        "    try:\n",
        "        errores_clasificados = {}  # Diccionario para almacenar las URLs seg√∫n su tipo de error\n",
        "\n",
        "        # üîπ Obtener las URLs desde el sitemap\n",
        "        urls_a_inspeccionar = obtener_urls_sitemap(sitemap_index_url)\n",
        "        print(f\"üîç Se encontraron {len(urls_a_inspeccionar)} URLs en el sitemap.\")\n",
        "\n",
        "        for url in urls_a_inspeccionar:\n",
        "            try:\n",
        "                request = {\"inspectionUrl\": url, \"siteUrl\": site_url}\n",
        "                response = service_gsc.urlInspection().index().inspect(body=request).execute()\n",
        "\n",
        "                # üìå Extraer el estado de indexaci√≥n\n",
        "                index_status = response[\"inspectionResult\"][\"indexStatusResult\"]\n",
        "                motivo = index_status.get(\"indexingState\", \"Desconocido\")\n",
        "\n",
        "                # Si es un error nuevo, creamos una nueva lista para almacenarlo\n",
        "                if motivo not in errores_clasificados:\n",
        "                    errores_clasificados[motivo] = []\n",
        "\n",
        "                # Agregar la URL a la categor√≠a de error correspondiente\n",
        "                errores_clasificados[motivo].append(url)\n",
        "\n",
        "                print(f\"üîç {url} ‚Üí {motivo}\")\n",
        "\n",
        "            except HttpError as e:\n",
        "                print(f\"‚ö†Ô∏è Error con {url}: {e}\")\n",
        "\n",
        "        # üîπ Guardar los errores en archivos separados\n",
        "        for motivo, urls in errores_clasificados.items():\n",
        "            nombre_archivo = motivo.replace(\" \", \"_\") + \".txt\"  # Formato de nombre del archivo\n",
        "            ruta_archivo = os.path.join(output_dir, nombre_archivo)\n",
        "\n",
        "            with open(ruta_archivo, \"w\") as f:\n",
        "                for url in urls:\n",
        "                    f.write(url + \"\\n\")\n",
        "\n",
        "            print(f\"‚úÖ {len(urls)} URLs guardadas en '{ruta_archivo}'\")\n",
        "\n",
        "        return errores_clasificados\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al obtener los errores de indexaci√≥n: {e}\")\n",
        "        return None\n",
        "\n",
        "# üîπ Ejecutar extracci√≥n y clasificaci√≥n de errores en un solo paso\n",
        "errores_clasificados = obtener_y_clasificar_errores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwkTi3g4ISTm",
        "outputId": "ad3b5c81-c854-4c8d-f493-b09253c3cce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Se encontraron 414 URLs en el sitemap.\n",
            "‚ùå Error al obtener los errores de indexaci√≥n: name 'site_url' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå PASO 4: Corregir Errores de Indexaci√≥n"
      ],
      "metadata": {
        "id": "Yv6O_GQUIdHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Corregir URLs 404\n",
        "\n",
        "üîπ Si la p√°gina da 404, podemos:\n",
        "\t‚Ä¢\tRevisar si la URL correcta est√° en el sitemap y redirigirla (301).\n",
        "\t‚Ä¢\tEnviar un 410 si la URL ya no existe y no debe ser indexada.\n",
        "\t‚Ä¢\tReemplazar en el sitemap por una versi√≥n correcta."
      ],
      "metadata": {
        "id": "gYcRfDBxIldX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Configurar credenciales de la API de Search Console\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "CREDENTIALS_FILE = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "SITE_URL = \"sc-domain:amazon-andes.com\"\n",
        "\n",
        "# üìå URLs base\n",
        "SITEMAP_INDEX_URL = \"https://amazon-andes.com/sitemap_index.xml\"\n",
        "WORDPRESS_API_URL = \"https://amazon-andes.com/wp-json/redirection/v1/redirect\"\n",
        "WORDPRESS_USER = \"admin\"\n",
        "WORDPRESS_PASSWORD = \"wfi6 lkKP 3P86 eeoD yBaa 6xKx\"\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Obtener todas las URLs del sitemap\n",
        "def obtener_urls_sitemap():\n",
        "    \"\"\"Obtiene todas las URLs de los sitemaps listados en el sitemap index.\"\"\"\n",
        "    urls_totales = set()\n",
        "\n",
        "    response = requests.get(SITEMAP_INDEX_URL)\n",
        "    if response.status_code == 200:\n",
        "        root = ET.fromstring(response.content)\n",
        "        sitemaps = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "\n",
        "        for sitemap in sitemaps:\n",
        "            response = requests.get(sitemap)\n",
        "            if response.status_code == 200:\n",
        "                root = ET.fromstring(response.content)\n",
        "                urls = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                urls_totales.update(urls)\n",
        "\n",
        "    return urls_totales\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Obtener URLs con error 404 desde Google Search Console\n",
        "def obtener_urls_404():\n",
        "    \"\"\"Obtiene las URLs reportadas como 'No se ha encontrado (404)' en Google Search Console.\"\"\"\n",
        "    credentials = service_account.Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=SCOPES)\n",
        "    service = build(\"webmasters\", \"v3\", credentials=credentials)\n",
        "\n",
        "    query = {\n",
        "        \"startDate\": \"2024-01-01\",\n",
        "        \"endDate\": \"2025-02-19\",\n",
        "        \"dimensions\": [\"page\"],\n",
        "        \"dimensionFilterGroups\": [{\n",
        "            \"filters\": [{\n",
        "                \"dimension\": \"page\",\n",
        "                \"operator\": \"contains\",\n",
        "                \"expression\": \"404\"\n",
        "            }]\n",
        "        }],\n",
        "        \"rowLimit\": 1000\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = service.searchanalytics().query(siteUrl=SITE_URL, body=query).execute()\n",
        "        if \"rows\" in response:\n",
        "            return {row[\"keys\"][0] for row in response[\"rows\"]}\n",
        "        else:\n",
        "            return set()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error al obtener las URLs 404 desde GSC: {e}\")\n",
        "        return set()\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Comparar ambas listas y generar redirecciones\n",
        "def agregar_redirecciones_en_wordpress():\n",
        "    \"\"\"Compara URLs del sitemap y GSC y crea redirecciones 301 en WordPress.\"\"\"\n",
        "    print(\"üîç Obteniendo URLs del sitemap...\")\n",
        "    urls_sitemap = obtener_urls_sitemap()\n",
        "    print(f\"‚úÖ {len(urls_sitemap)} URLs extra√≠das del sitemap.\")\n",
        "\n",
        "    print(\"üîç Obteniendo URLs con error 404 desde GSC...\")\n",
        "    urls_404 = obtener_urls_404()\n",
        "    print(f\"‚ùå {len(urls_404)} URLs con error 404 encontradas en GSC.\")\n",
        "\n",
        "    urls_para_redirigir = urls_404 - urls_sitemap\n",
        "    print(f\"üîÅ {len(urls_para_redirigir)} URLs ser√°n redirigidas a la p√°gina principal.\")\n",
        "\n",
        "    if urls_para_redirigir:\n",
        "        for url in urls_para_redirigir:\n",
        "            data = {\n",
        "                \"source\": url.replace(SITE_URL, \"\"),  # WordPress guarda rutas relativas\n",
        "                \"target\": SITE_URL,\n",
        "                \"type\": \"301\",\n",
        "                \"match\": \"url\",\n",
        "                \"status\": \"enabled\"\n",
        "            }\n",
        "\n",
        "            response = requests.post(WORDPRESS_API_URL, json=data, auth=(WORDPRESS_USER, WORDPRESS_PASSWORD))\n",
        "\n",
        "            if response.status_code == 201:\n",
        "                print(f\"‚úÖ Redirecci√≥n a√±adida: {url} ‚ûù {SITE_URL}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Error al agregar {url}: {response.text}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No hay URLs nuevas para redirigir.\")\n",
        "\n",
        "# üîπ Ejecutar el script\n",
        "agregar_redirecciones_en_wordpress()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1XSKC7Fg6tz",
        "outputId": "d402878a-fc95-4f69-9c0e-a6dc1a92ae65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Obteniendo URLs del sitemap...\n",
            "‚úÖ 415 URLs extra√≠das del sitemap.\n",
            "üîç Obteniendo URLs con error 404 desde GSC...\n",
            "‚ùå 1 URLs con error 404 encontradas en GSC.\n",
            "üîÅ 1 URLs ser√°n redirigidas a la p√°gina principal.\n",
            "‚ö†Ô∏è Error al agregar https://ru.amazon-andes.com/product/tocosh-capsules-100-x-400-mg/?add-to-cart=404: {\"code\":\"invalid_username\",\"message\":\"<strong>Error:<\\/strong> Unknown username. Check again or try your email address.\",\"data\":{\"status\":401}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import time\n",
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Configuraci√≥n de la API de GSC\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "CREDENTIALS_FILE = \"/content/drive/MyDrive/000 | Amazon Andes/indexing-cs-amazon-andes.json\"\n",
        "SITE_URL = \"sc-domain:amazon-andes.com\"  # ‚ö†Ô∏è Usa la URL exacta registrada en GSC\n",
        "\n",
        "# üìå Archivo CSV de salida\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/000 | Amazon Andes/errores_404.csv\"\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Conectar con Google Search Console API\n",
        "def conectar_gsc():\n",
        "    \"\"\"Autentica con la API de Google Search Console y devuelve el servicio.\"\"\"\n",
        "    credentials = service_account.Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=SCOPES)\n",
        "    return build(\"webmasters\", \"v3\", credentials=credentials)\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Obtener las URLs con error 404 en \"Cobertura\"\n",
        "def obtener_urls_404():\n",
        "    \"\"\"Consulta Google Search Console y obtiene las URLs con error 404.\"\"\"\n",
        "    service = conectar_gsc()\n",
        "\n",
        "    try:\n",
        "        query = {\n",
        "            \"startDate\": \"2024-01-01\",  # Fecha de inicio del an√°lisis\n",
        "            \"endDate\": \"2024-12-31\",    # Fecha de fin del an√°lisis\n",
        "            \"dimensions\": [\"page\"],\n",
        "            \"dimensionFilterGroups\": [{\n",
        "                \"filters\": [{\n",
        "                    \"dimension\": \"page\",\n",
        "                    \"operator\": \"contains\",\n",
        "                    \"expression\": \"404\"\n",
        "                }]\n",
        "            }],\n",
        "            \"rowLimit\": 1000  # M√°ximo de URLs a obtener en una consulta\n",
        "        }\n",
        "\n",
        "        response = service.searchanalytics().query(siteUrl=SITE_URL, body=query).execute()\n",
        "\n",
        "        urls_404 = [row[\"keys\"][0] for row in response.get(\"rows\", [])]\n",
        "        return urls_404\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error al obtener las URLs 404 desde GSC: {e}\")\n",
        "        return []\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Guardar los errores en un archivo CSV\n",
        "def guardar_csv(urls_404):\n",
        "    \"\"\"Guarda las URLs con error 404 en un archivo CSV.\"\"\"\n",
        "    if urls_404:\n",
        "        with open(OUTPUT_CSV, \"w\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"URL 404\"])  # Encabezado\n",
        "            for url in urls_404:\n",
        "                writer.writerow([url])\n",
        "        print(f\"‚úÖ {len(urls_404)} URLs 404 guardadas en '{OUTPUT_CSV}'.\")\n",
        "    else:\n",
        "        print(\"‚úÖ No se encontraron errores 404 en GSC.\")\n",
        "\n",
        "# üîπ Ejecutar el script\n",
        "urls_404 = obtener_urls_404()\n",
        "guardar_csv(urls_404)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIJIBP3fn1Hk",
        "outputId": "0c7439c7-b6fc-4778-8f56-ce97337cdc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 1 URLs 404 guardadas en '/content/drive/MyDrive/000 | Amazon Andes/errores_404.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def corregir_errores_404(df_errores):\n",
        "    \"\"\"Revisa URLs con error 404 y aplica soluciones\"\"\"\n",
        "    urls_404 = df_errores[df_errores[\"Motivo\"] == \"No se ha encontrado (404)\"][\"URL\"]\n",
        "\n",
        "    for url in urls_404:\n",
        "        print(f\"üîç Revisando: {url}\")\n",
        "\n",
        "        # üîπ Verificar si la URL correcta existe en el sitemap\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ La URL {url} ahora es accesible. Enviar a indexar.\")\n",
        "            enviar_url_para_indexacion(url)  # Funci√≥n definida en el Script 2\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è La URL {url} sigue sin existir. Considera un redireccionamiento 301 o 410.\")\n",
        "\n",
        "# Ejecutar correcci√≥n de 404\n",
        "corregir_errores_404(df_errores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "QYRgBqShImp2",
        "outputId": "12c8055c-4fe6-47ef-dd6c-bfe142b88129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Motivo'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Motivo'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0f261642951d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Ejecutar correcci√≥n de 404\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcorregir_errores_404\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_errores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-0f261642951d>\u001b[0m in \u001b[0;36mcorregir_errores_404\u001b[0;34m(df_errores)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorregir_errores_404\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_errores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Revisa URLs con error 404 y aplica soluciones\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0murls_404\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_errores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_errores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Motivo\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"No se ha encontrado (404)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"URL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls_404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Motivo'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Corregir Errores de Can√≥nicas Incorrectas\n",
        "\n",
        "üîπ Si la p√°gina tiene un rel=canonical incorrecto:\n",
        "\t‚Ä¢\tActualizar el rel=canonical en el HTML de la p√°gina afectada.\n",
        "\t‚Ä¢\tAsegurar que la URL can√≥nica apunta a la p√°gina correcta."
      ],
      "metadata": {
        "id": "IF385690IsE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corregir_errores_canonicas(df_errores):\n",
        "    \"\"\"Revisa URLs con problemas de etiqueta can√≥nica\"\"\"\n",
        "    urls_canonicas = df_errores[df_errores[\"Motivo\"] == \"P√°gina alternativa con etiqueta can√≥nica adecuada\"][\"URL\"]\n",
        "\n",
        "    for url in urls_canonicas:\n",
        "        print(f\"üîç Revisando: {url}\")\n",
        "        # Aqu√≠ se podr√≠a hacer un request y validar el contenido de <link rel=\"canonical\">\n",
        "        # Pero lo ideal es revisar manualmente en los archivos fuente del sitio\n",
        "        print(f\"‚ö†Ô∏è Verificar manualmente la etiqueta canonical en: {url}\")\n",
        "\n",
        "# Ejecutar revisi√≥n de etiquetas can√≥nicas\n",
        "corregir_errores_canonicas(df_errores)"
      ],
      "metadata": {
        "id": "JyoZvpwyItZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3Ô∏è‚É£ Corregir Bloqueos 4xx\n",
        "\n",
        "üîπ Si la p√°gina est√° bloqueada por robots.txt, 403, 401 o 410:\n",
        "\t‚Ä¢\tRevisar las reglas en robots.txt.\n",
        "\t‚Ä¢\tQuitar restricciones innecesarias en el .htaccess o server.conf.\n",
        "\t‚Ä¢\tPermitir el acceso a Googlebot en la configuraci√≥n del servidor."
      ],
      "metadata": {
        "id": "JObDu1kNIv3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corregir_errores_4xx(df_errores):\n",
        "    \"\"\"Revisa URLs bloqueadas por errores 4xx\"\"\"\n",
        "    urls_bloqueadas = df_errores[df_errores[\"Motivo\"].str.contains(\"4xx\")][\"URL\"]\n",
        "\n",
        "    for url in urls_bloqueadas:\n",
        "        print(f\"üîç Revisando acceso a: {url}\")\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ La URL {url} ahora es accesible. Enviar a indexar.\")\n",
        "            enviar_url_para_indexacion(url)  # Funci√≥n definida en el Script 2\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è La URL {url} sigue bloqueada. Revisar robots.txt o restricciones del servidor.\")\n",
        "\n",
        "# Ejecutar correcci√≥n de bloqueos 4xx\n",
        "corregir_errores_4xx(df_errores)"
      ],
      "metadata": {
        "id": "Snp4P2_xIxhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå PASO 5: Enviar URLs Corregidas a Google Indexing API\n",
        "\n",
        "Despu√©s de solucionar los problemas, podemos reenviar las URLs corregidas a Google Indexing API para que sean reindexadas m√°s r√°pido.\n",
        "\n",
        "üîπ Usa el Script 2 que ya creamos para enviar URLs a Google Indexing API."
      ],
      "metadata": {
        "id": "nHO8uYphI1Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for url in df_errores[\"URL\"]:\n",
        "    enviar_url_para_indexacion(url)"
      ],
      "metadata": {
        "id": "51xN5dtbI2ef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}