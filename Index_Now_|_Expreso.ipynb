{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zauqaIT52260",
        "bVw9uwUzDDoz",
        "hMTgSphZReYY",
        "mfhICoZaIBtQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eespejoruiz/IndexNow/blob/main/Index_Now_%7C_Expreso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) API Search Console Index Now\n"
      ],
      "metadata": {
        "id": "POMSn8A27gZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install git+https://github.com/antoineeripret/gsc_wrapper"
      ],
      "metadata": {
        "id": "TpRBn1BDzkf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Instalo Librer√≠as"
      ],
      "metadata": {
        "id": "csJ5UTUb7n8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgb54FDQymyb"
      },
      "outputs": [],
      "source": [
        "import gscwrapper\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gscwrapper.account import Account"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show gscwrapper"
      ],
      "metadata": {
        "id": "I-VaHdLf6nM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Me conecto a google drive\n"
      ],
      "metadata": {
        "id": "zauqaIT52260"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Mp6UXgcv28Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Me Conecto al API y Selecciono de Propiedad en Google Search Console\n"
      ],
      "metadata": {
        "id": "GPjsPsaf8Wa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account_service = (\n",
        "    gscwrapper\n",
        "    .generate_auth(\n",
        "        client_config=\"/content/drive/MyDrive/000 _ Expreso/search-console-450718-997bc7dc75d1.json\",\n",
        "        service_account_auth=True\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "vdtz5rZTUCxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 _ Expreso/indexing cs.json\"\n",
        "\n",
        "# üìå Definir el `scope` correcto para la API de Indexing\n",
        "SCOPES = [\"https://www.googleapis.com/auth/indexing\"]\n",
        "\n",
        "def autenticar_con_cuenta_de_servicio():\n",
        "    \"\"\"Autentica con una cuenta de servicio sin intervenci√≥n manual.\"\"\"\n",
        "\n",
        "    creds = service_account.Credentials.from_service_account_file(\n",
        "        credenciales_json,\n",
        "        scopes=SCOPES\n",
        "    )\n",
        "\n",
        "    return creds\n",
        "\n",
        "# üîπ Ejecutar la autenticaci√≥n con la cuenta de servicio\n",
        "credentials = autenticar_con_cuenta_de_servicio()\n",
        "\n",
        "# üîπ Crear servicio para la API de Indexing con los permisos correctos\n",
        "service = build(\"indexing\", \"v3\", credentials=credentials)\n",
        "\n",
        "print(\"‚úÖ Autenticaci√≥n exitosa con cuenta de servicio. 'service' est√° listo para Indexing API.\")"
      ],
      "metadata": {
        "id": "GPWND5KCV8P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "account_service"
      ],
      "metadata": {
        "id": "05Ns75RUUc77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Extraer URLs del Sitemap"
      ],
      "metadata": {
        "id": "-zJYI5nZ8Lrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import gzip\n",
        "import io\n",
        "import time\n",
        "\n",
        "# üìå URLs base\n",
        "sitemap_index_url = \"https://www.expreso.ec/sitemap-index.xml\"\n",
        "ruta_indexadas = \"/content/drive/MyDrive/000 _ Expreso/urls_indexadas.txt\"\n",
        "ruta_por_indexar = \"/content/drive/MyDrive/000 _ Expreso/urls_por_indexar.txt\"\n",
        "\n",
        "def obtener_sitemaps(url):\n",
        "    \"\"\"Obtiene la lista de archivos sitemap desde el sitemap index con reintentos en caso de fallo.\"\"\"\n",
        "    for intento in range(3):  # Reintentar hasta 3 veces\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Verificar si el contenido es XML v√°lido\n",
        "            if \"xml\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "                root = ET.fromstring(response.content)\n",
        "                sitemaps = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                return sitemaps\n",
        "            else:\n",
        "                print(\"‚ùå El contenido recibido no es XML v√°lido.\")\n",
        "                return []\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ö†Ô∏è Error al obtener el sitemap index. Intento {intento + 1}/3: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(\"‚ùå No se pudo obtener el sitemap index despu√©s de 3 intentos.\")\n",
        "    return []\n",
        "\n",
        "def obtener_urls_de_sitemaps(lista_sitemaps):\n",
        "    \"\"\"Escanea todos los sitemaps y extrae las URLs, descomprimiendo archivos .gz si es necesario.\"\"\"\n",
        "    urls_totales = set()\n",
        "\n",
        "    for sitemap in lista_sitemaps:\n",
        "        for intento in range(3):  # Reintentar hasta 3 veces en caso de error\n",
        "            try:\n",
        "                response = requests.get(sitemap, timeout=10)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Si el archivo es .xml.gz, descomprimirlo antes de procesarlo\n",
        "                if sitemap.endswith(\".gz\"):\n",
        "                    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
        "                        xml_content = f.read().decode(\"utf-8\")\n",
        "                else:\n",
        "                    xml_content = response.content.decode(\"utf-8\")\n",
        "\n",
        "                # Validar si el XML es v√°lido\n",
        "                if not xml_content.strip().startswith(\"<\"):\n",
        "                    print(f\"‚ö†Ô∏è {sitemap} no contiene XML v√°lido.\")\n",
        "                    continue\n",
        "\n",
        "                # Parsear el XML\n",
        "                root = ET.fromstring(xml_content)\n",
        "                urls = [elem.text for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
        "                urls_totales.update(urls)\n",
        "                break  # Salir del bucle si el intento fue exitoso\n",
        "\n",
        "            except (requests.exceptions.RequestException, ET.ParseError) as e:\n",
        "                print(f\"‚ö†Ô∏è Error en intento {intento + 1}/3 con {sitemap}: {e}\")\n",
        "                time.sleep(2)  # Esperar antes de reintentar\n",
        "\n",
        "    return urls_totales\n",
        "\n",
        "def leer_urls_indexadas(ruta):\n",
        "    \"\"\"Lee las URLs ya indexadas desde el archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return set(f.read().splitlines())\n",
        "    return set()\n",
        "\n",
        "def guardar_urls_nuevas(ruta, urls_nuevas):\n",
        "    \"\"\"Guarda solo las nuevas URLs en el archivo (sobrescribe el archivo anterior).\"\"\"\n",
        "    if urls_nuevas:\n",
        "        with open(ruta, \"w\") as f:  # ‚ö†Ô∏è 'w' para sobrescribir el archivo anterior\n",
        "            for url in urls_nuevas:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚úÖ {len(urls_nuevas)} nuevas URLs guardadas en '{ruta}'.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No hay nuevas URLs para agregar.\")\n",
        "\n",
        "def actualizar_urls_indexadas(ruta, urls_nuevas):\n",
        "    \"\"\"Agrega las nuevas URLs indexadas al archivo `urls_indexadas.txt`.\"\"\"\n",
        "    if urls_nuevas:\n",
        "        with open(ruta, \"a\") as f:  # ‚ö†Ô∏è 'a' para agregar nuevas URLs sin borrar las anteriores\n",
        "            for url in urls_nuevas:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚úÖ {len(urls_nuevas)} URLs a√±adidas a '{ruta}' para futuras referencias.\")\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Obtener todos los archivos de sitemap desde el sitemap index\n",
        "sitemaps_encontrados = obtener_sitemaps(sitemap_index_url)\n",
        "print(f\"üîç Se encontraron {len(sitemaps_encontrados)} sitemaps.\")\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Extraer todas las URLs de los sitemaps\n",
        "urls_extraidas = obtener_urls_de_sitemaps(sitemaps_encontrados)\n",
        "print(f\"‚úÖ Total de URLs extra√≠das: {len(urls_extraidas)}\")\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Leer el archivo de URLs indexadas y filtrar las nuevas\n",
        "urls_indexadas = leer_urls_indexadas(ruta_indexadas)\n",
        "urls_nuevas = urls_extraidas - urls_indexadas\n",
        "\n",
        "# üîπ 4Ô∏è‚É£ Guardar solo las nuevas URLs en `urls_por_indexar.txt` para ser enviadas\n",
        "guardar_urls_nuevas(ruta_por_indexar, urls_nuevas)\n",
        "\n",
        "# üîπ 5Ô∏è‚É£ Actualizar `urls_indexadas.txt` con las nuevas URLs enviadas\n",
        "actualizar_urls_indexadas(ruta_indexadas, urls_nuevas)"
      ],
      "metadata": {
        "id": "VZXAFt_D3XhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Enviar las URLs Nuevas a Google Search Console"
      ],
      "metadata": {
        "id": "LDT7hQqiSbv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "# üìå Ruta del archivo donde est√°n las URLs a indexar\n",
        "ruta_urls = \"/content/drive/MyDrive/000 _ Expreso/urls_por_indexar.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 _ Expreso/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo y las devuelve como una lista.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google y muestra el estado del env√≠o.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "\n",
        "        # üìå Verificar si la API respondi√≥ correctamente\n",
        "        if \"urlNotificationMetadata\" in response:\n",
        "            print(f\"‚úÖ URL enviada con √©xito: {url}\")\n",
        "            print(f\"üîπ √öltima actualizaci√≥n: {response['urlNotificationMetadata'].get('latestUpdate', 'No disponible')}\\n\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è URL enviada pero sin confirmaci√≥n clara: {url}\")\n",
        "            print(f\"üîπ Respuesta: {response}\\n\")\n",
        "\n",
        "        return True  # Indica que la URL se envi√≥ correctamente\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\\n\")\n",
        "        return False  # Indica que hubo un error\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que no pudieron enviarse para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"w\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "# üîπ Leer las URLs desde el archivo\n",
        "urls_a_indexar = leer_urls(ruta_urls)\n",
        "\n",
        "if not urls_a_indexar:\n",
        "    print(\"‚ö†Ô∏è No hay URLs en el archivo para indexar.\")\n",
        "else:\n",
        "    print(f\"üöÄ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "\n",
        "    # üîπ Lista de URLs que fallaron al enviarse\n",
        "    urls_fallidas = []\n",
        "\n",
        "    # üîπ Enviar todas las URLs a Google Indexing API\n",
        "    for url in urls_a_indexar:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_fallidas.append(url)\n",
        "\n",
        "        time.sleep(2)  # üîπ Pausa para evitar bloqueos\n",
        "\n",
        "    # üîπ Guardar las URLs con errores para reintento\n",
        "    guardar_urls_fallidas(urls_fallidas)\n",
        "\n",
        "    print(\"‚úÖ Proceso de env√≠o completado.\")"
      ],
      "metadata": {
        "id": "mZxOBAUbSfhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî∫ Enviar las TODAS las URLs a Google Search Console | Usar solo en casos de cambios generales en la web."
      ],
      "metadata": {
        "id": "bVw9uwUzDDoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Archivos de URLs\n",
        "ruta_urls = \"/content/drive/MyDrive/000 _ Expreso/urls_indexadas.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 _ Expreso/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que fallaron en un archivo para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"a\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada: {url} - Respuesta: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Leer todas las URLs desde el archivo\n",
        "urls_a_indexar = leer_urls(ruta_urls)\n",
        "\n",
        "# Filtrar para enviar solo 2000 por d√≠a\n",
        "urls_a_indexar = urls_a_indexar[:2000]\n",
        "\n",
        "# Lista de URLs que no pudieron enviarse\n",
        "urls_fallidas = []\n",
        "\n",
        "if urls_a_indexar:\n",
        "    print(f\"üîπ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "    for url in urls_a_indexar:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_fallidas.append(url)\n",
        "        time.sleep(2)  # Pausa para evitar bloqueos\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay URLs en el archivo para indexar.\")\n",
        "\n",
        "# Guardar las URLs con errores\n",
        "guardar_urls_fallidas(urls_fallidas)"
      ],
      "metadata": {
        "id": "N_loQkaiCj5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå C√≥digo: Reintentar el env√≠o de URLs fallidas"
      ],
      "metadata": {
        "id": "hMTgSphZReYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Archivo con las URLs que fallaron en intentos anteriores\n",
        "ruta_errores = \"/content/drive/MyDrive/000 _ Expreso/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls(ruta):\n",
        "    \"\"\"Lee las URLs desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Sobrescribe el archivo con las URLs que a√∫n fallan.\"\"\"\n",
        "    with open(ruta_errores, \"w\") as f:\n",
        "        for url in urls:\n",
        "            f.write(url + \"\\n\")\n",
        "    print(f\"‚ö†Ô∏è {len(urls)} URLs siguen fallando y se guardaron en '{ruta_errores}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada: {url} - Respuesta: {response}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Leer las URLs fallidas\n",
        "urls_fallidas = leer_urls(ruta_errores)\n",
        "\n",
        "# Lista de URLs que sigan fallando despu√©s del reintento\n",
        "urls_errores_nuevos = []\n",
        "\n",
        "if urls_fallidas:\n",
        "    print(f\"üîπ Reintentando enviar {len(urls_fallidas)} URLs a Google Indexing API...\")\n",
        "    for url in urls_fallidas:\n",
        "        if not enviar_url_para_indexacion(url):\n",
        "            urls_errores_nuevos.append(url)  # Guardar solo las que siguen fallando\n",
        "        time.sleep(2)  # Pausa para evitar bloqueos\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay URLs fallidas para reintentar.\")\n",
        "\n",
        "# Guardar las URLs que a√∫n fallan\n",
        "guardar_urls_fallidas(urls_errores_nuevos)"
      ],
      "metadata": {
        "id": "hSh5mEXBRg1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ Verificar y Enviar a Indexar las URLs No Indexadas"
      ],
      "metadata": {
        "id": "mfhICoZaIBtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client requests"
      ],
      "metadata": {
        "id": "ZlO-b7YzIDcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 | Expreso/indexing cs.json\"\n",
        "\n",
        "# üìå Definir los permisos (scopes) correctos\n",
        "SCOPES_GSC = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "SCOPES_INDEXING = [\"https://www.googleapis.com/auth/indexing\"]\n",
        "\n",
        "# üîπ Autenticaci√≥n con Google Search Console API\n",
        "credentials_gsc = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES_GSC)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials_gsc)\n",
        "\n",
        "# üîπ Autenticaci√≥n con Google Indexing API\n",
        "credentials_indexing = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES_INDEXING)\n",
        "service_indexing = build(\"indexing\", \"v3\", credentials=credentials_indexing)\n",
        "\n",
        "print(\"‚úÖ Autenticaci√≥n exitosa con Google Search Console API y Google Indexing API\")"
      ],
      "metadata": {
        "id": "0TgRTVxcINl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 _ Expreso/indexing cs.json\"\n",
        "\n",
        "# üìå Definir los permisos (scopes) correctos\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "\n",
        "# üîπ Autenticar con Google Search Console API\n",
        "credentials = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "\n",
        "# üîπ Obtener la lista de sitios que la cuenta de servicio puede acceder\n",
        "sites_list = service_gsc.sites().list().execute()\n",
        "\n",
        "# üîπ Mostrar los sitios a los que tiene acceso\n",
        "for site in sites_list.get(\"siteEntry\", []):\n",
        "    print(f\"üîπ Acceso a: {site['siteUrl']} - Rol: {site['permissionLevel']}\")"
      ],
      "metadata": {
        "id": "OcCCruZASofY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "from io import BytesIO\n",
        "\n",
        "# üìå URL del sitemap indexado\n",
        "sitemap_index_url = \"https://www.expreso.ec/sitemap-index.xml\"\n",
        "\n",
        "# üìå Archivos de salida\n",
        "output_csv = \"/content/drive/MyDrive/000 _ Expreso/resultados_indexacion.csv\"\n",
        "urls_no_indexadas_txt = \"/content/drive/MyDrive/000 _ Expreso/urls_no_indexadas.txt\"\n",
        "\n",
        "# üîπ 1Ô∏è‚É£ Obtener todos los sitemaps desde `sitemap-index.xml`\n",
        "def obtener_sitemaps(url_sitemap_index):\n",
        "    response = requests.get(url_sitemap_index)\n",
        "    sitemaps = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            root = ET.fromstring(response.content)\n",
        "            for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
        "                sitemaps.append(elem.text)\n",
        "        except ET.ParseError:\n",
        "            print(\"‚ùå Error al analizar el sitemap indexado. Verifica que sea XML v√°lido.\")\n",
        "    else:\n",
        "        print(\"‚ùå Error al obtener el sitemap indexado\")\n",
        "\n",
        "    return sitemaps\n",
        "\n",
        "# üîπ 2Ô∏è‚É£ Obtener todas las URLs de cada sitemap (maneja archivos .xml y .xml.gz)\n",
        "def obtener_urls_de_sitemaps(lista_sitemaps):\n",
        "    urls_totales = []\n",
        "\n",
        "    for sitemap in lista_sitemaps:\n",
        "        response = requests.get(sitemap)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                if sitemap.endswith(\".gz\"):  # üìå Si es un archivo comprimido, lo descomprimimos\n",
        "                    with gzip.GzipFile(fileobj=BytesIO(response.content)) as f:\n",
        "                        content = f.read()\n",
        "                else:\n",
        "                    content = response.content\n",
        "\n",
        "                root = ET.fromstring(content)\n",
        "                for elem in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
        "                    urls_totales.append(elem.text)\n",
        "\n",
        "            except ET.ParseError:\n",
        "                print(f\"‚ö†Ô∏è Error al analizar {sitemap}. Verifica que sea XML v√°lido.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ùå Error al obtener {sitemap}\")\n",
        "\n",
        "    return urls_totales\n",
        "\n",
        "# üîπ 3Ô∏è‚É£ Consultar si una URL est√° indexada en Google Search Console\n",
        "def verificar_indexacion(service_gsc, url, site_url):\n",
        "    try:\n",
        "        request = {\n",
        "            \"startDate\": \"2024-01-01\",\n",
        "            \"endDate\": \"2025-02-18\",\n",
        "            \"dimensions\": [\"page\"],\n",
        "            \"dimensionFilterGroups\": [{\n",
        "                \"filters\": [{\n",
        "                    \"dimension\": \"page\",\n",
        "                    \"operator\": \"equals\",\n",
        "                    \"expression\": url\n",
        "                }]\n",
        "            }],\n",
        "            \"rowLimit\": 1\n",
        "        }\n",
        "\n",
        "        # üîπ Consultar la API de GSC\n",
        "        response = service_gsc.searchanalytics().query(siteUrl=site_url, body=request).execute()\n",
        "\n",
        "        if \"rows\" in response and len(response[\"rows\"]) > 0:\n",
        "            return \"‚úÖ Indexada\"\n",
        "        else:\n",
        "            return \"‚ùå No indexada\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error con {url}: {e}\")\n",
        "        return \"‚ö†Ô∏è Error\"\n",
        "\n",
        "# üîπ 4Ô∏è‚É£ Ejecutar la verificaci√≥n de indexaci√≥n\n",
        "def analizar_indexacion():\n",
        "    site_url = \"sc-domain:expreso.ec\"  # ‚ö†Ô∏è Usa la versi√≥n exacta registrada en GSC\n",
        "    print(f\"üîπ Obteniendo lista de sitemaps desde: {sitemap_index_url}\")\n",
        "    lista_sitemaps = obtener_sitemaps(sitemap_index_url)\n",
        "\n",
        "    urls_sitemap = obtener_urls_de_sitemaps(lista_sitemaps)\n",
        "    resultados = []\n",
        "    urls_no_indexadas = []\n",
        "\n",
        "    print(f\"üîπ Verificando indexaci√≥n de {len(urls_sitemap)} URLs en Google Search Console...\")\n",
        "\n",
        "    for url in urls_sitemap:\n",
        "        estado = verificar_indexacion(service_gsc, url, site_url)\n",
        "        resultados.append([url, estado])\n",
        "\n",
        "        if estado == \"‚ùå No indexada\":\n",
        "            urls_no_indexadas.append(url)\n",
        "\n",
        "        time.sleep(2)  # üîπ Pausa para evitar l√≠mites de la API\n",
        "\n",
        "    # Guardar los resultados\n",
        "    with open(output_csv, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"URL\", \"Estado de Indexaci√≥n\"])\n",
        "        writer.writerows(resultados)\n",
        "\n",
        "    with open(urls_no_indexadas_txt, \"w\") as f:\n",
        "        for url in urls_no_indexadas:\n",
        "            f.write(url + \"\\n\")\n",
        "\n",
        "    print(\"‚úÖ An√°lisis de indexaci√≥n completado.\")\n",
        "\n",
        "# üîπ Ejecutar la verificaci√≥n de indexaci√≥n\n",
        "analizar_indexacion()"
      ],
      "metadata": {
        "id": "d-pZtVQHIWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# üìå Ruta del archivo JSON de la cuenta de servicio en Google Drive\n",
        "credenciales_json = \"/content/drive/MyDrive/000 _ Expreso/indexing cs.json\"\n",
        "\n",
        "# üìå Definir el permiso (scope) correcto\n",
        "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
        "\n",
        "# üîπ Autenticar con Google Search Console API\n",
        "credentials = service_account.Credentials.from_service_account_file(credenciales_json, scopes=SCOPES)\n",
        "service_gsc = build(\"searchconsole\", \"v1\", credentials=credentials)\n",
        "\n",
        "# üîπ Obtener la lista de sitios que la cuenta de servicio puede acceder\n",
        "sites_list = service_gsc.sites().list().execute()\n",
        "\n",
        "# üîπ Mostrar los sitios a los que tiene acceso\n",
        "print(\"üîπ La cuenta de servicio tiene acceso a:\")\n",
        "for site in sites_list.get(\"siteEntry\", []):\n",
        "    print(f\" - {site['siteUrl']} | Permiso: {site['permissionLevel']}\")"
      ],
      "metadata": {
        "id": "QL4a6IBCS0vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# üìå Archivos de entrada/salida\n",
        "ruta_urls_no_indexadas = \"/content/drive/MyDrive/000 _ Expreso/urls_no_indexadas.txt\"\n",
        "ruta_errores = \"/content/drive/MyDrive/000 _ Expreso/urls_fallidas.txt\"\n",
        "\n",
        "def leer_urls_no_indexadas(ruta):\n",
        "    \"\"\"Lee las URLs no indexadas desde un archivo.\"\"\"\n",
        "    if os.path.exists(ruta):\n",
        "        with open(ruta, \"r\") as f:\n",
        "            return [line.strip() for line in f.readlines() if line.strip()]\n",
        "    return []\n",
        "\n",
        "def guardar_urls_fallidas(urls):\n",
        "    \"\"\"Guarda las URLs que fallaron en un archivo para reintentos futuros.\"\"\"\n",
        "    if urls:\n",
        "        with open(ruta_errores, \"w\") as f:\n",
        "            for url in urls:\n",
        "                f.write(url + \"\\n\")\n",
        "        print(f\"‚ö†Ô∏è {len(urls)} URLs con error guardadas en '{ruta_errores}'.\")\n",
        "\n",
        "def actualizar_archivo_urls(urls_exitosas):\n",
        "    \"\"\"Elimina del archivo las URLs que ya fueron indexadas correctamente.\"\"\"\n",
        "    if not os.path.exists(ruta_urls_no_indexadas):\n",
        "        return\n",
        "\n",
        "    urls_actuales = leer_urls_no_indexadas(ruta_urls_no_indexadas)\n",
        "    nuevas_urls = list(set(urls_actuales) - set(urls_exitosas))  # üîπ Filtrar URLs exitosas\n",
        "\n",
        "    with open(ruta_urls_no_indexadas, \"w\") as f:\n",
        "        for url in nuevas_urls:\n",
        "            f.write(url + \"\\n\")\n",
        "\n",
        "    print(f\"üìÑ Archivo actualizado: {len(nuevas_urls)} URLs restantes en '{ruta_urls_no_indexadas}'.\")\n",
        "\n",
        "def enviar_url_para_indexacion(url):\n",
        "    \"\"\"Env√≠a una URL a la API de Indexing de Google.\"\"\"\n",
        "    request_body = {\n",
        "        \"url\": url,\n",
        "        \"type\": \"URL_UPDATED\"\n",
        "    }\n",
        "    try:\n",
        "        response = service_indexing.urlNotifications().publish(body=request_body).execute()\n",
        "        print(f\"‚úÖ URL enviada a indexar: {url}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al enviar {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "# üîπ Leer URLs a indexar\n",
        "urls_a_indexar = leer_urls_no_indexadas(ruta_urls_no_indexadas)\n",
        "urls_fallidas = []\n",
        "urls_indexadas = []\n",
        "\n",
        "if urls_a_indexar:\n",
        "    print(f\"üîπ Enviando {len(urls_a_indexar)} URLs a Google Indexing API...\")\n",
        "    for url in urls_a_indexar:\n",
        "        if enviar_url_para_indexacion(url):\n",
        "            urls_indexadas.append(url)  # Guardamos las exitosas\n",
        "        else:\n",
        "            urls_fallidas.append(url)  # Guardamos las fallidas\n",
        "        time.sleep(2)  # üîπ Evitar bloqueos con la API\n",
        "\n",
        "# üîπ Actualizar archivos\n",
        "guardar_urls_fallidas(urls_fallidas)\n",
        "actualizar_archivo_urls(urls_indexadas)"
      ],
      "metadata": {
        "id": "JRpkC0daS7oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}